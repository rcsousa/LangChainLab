{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.chat_models import AzureChatOpenAI as AzureOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    " # Define OpenAI credentials\n",
    "openai_api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai_api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_deployment_id = os.getenv(\"MODEL_DEPLOYMENT_ID\")\n",
    "# Set environment variables\n",
    "if openai_api_base is not None:\n",
    "    os.environ[\"OPENAI_API_BASE\"] = openai_api_base\n",
    "if openai_api_key is not None:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "if openai_api_type is not None:\n",
    "    os.environ[\"OPENAI_API_TYPE\"] = openai_api_type\n",
    "if openai_api_version is not None:\n",
    "     os.environ[\"OPENAI_API_VERSION\"] = str(openai_api_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "client=any,\n",
    "deployment='embeddings',\n",
    "model='text-embedding-ada-002',\n",
    "openai_api_type=\"azure\",\n",
    "openai_api_base=\"https://copilot-openai.openai.azure.com\",\n",
    "openai_api_version=\"2023-05-15\",\n",
    "openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "chunk_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_docs = FAISS.load_local(\"./faiss_uploaded_docs\", embeddings)\n",
    "llm1 = AzureOpenAI(deployment_name='trouble-buddy', model_name='gpt-3.5-turbo', temperature=0, client=any)\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "qa1 = RetrievalQA.from_chain_type(\n",
    "    llm=llm1,\n",
    "    retriever=uploaded_docs.as_retriever(),\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_sre = FAISS.load_local(\"./site_reliability_engineering\", embeddings)\n",
    "#llm = AzureOpenAI(model_kwargs={'engine':'trouble-buddy'}, client=any)\n",
    "llm = AzureOpenAI(deployment_name='trouble-buddy', model_name='gpt-3.5-turbo', temperature=0.0, client=any)\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=kb_sre.as_retriever(),\n",
    "    chain_type='stuff',\n",
    "    memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_docs.similarity_search(\"What happened on Elastic Cloud?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa1.run(\"What happened?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa1.run(\"for how long did the incident last?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Documentos enviados\",\n",
    "        func=qa1.run,\n",
    "        description=(\n",
    "            'Use this tool to answer general purpose questions'\n",
    "        )\n",
    "    ),  \n",
    "    Tool(\n",
    "        name=\"Base de Conhecimento sobre o livro Building Secure and Reliable Systems do Google\",\n",
    "        func=qa.run,\n",
    "        description=(\n",
    "            'Only Use this tool when the user asks to include Site Reliability Engineering (SRE) topics as part of the answer'\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "Complete the objective as best you can. You have access to the following tools:\n",
    "- \"Documentos enviados\"\n",
    "- \"Base de Conhecimento sobre o livro Building Secure and Reliable Systems do Google\"\n",
    "- \"Base de Conhecimento sobre o livro Site Reliability Engineering do Google\"\n",
    "The user question is: {user_input}\n",
    "\n",
    "You are helpful and cheerful assistant who is capable of providing insightful and precise answers to user's question based on the information available in the faiss_uploaded_docs index.\n",
    "You try to be as helpeful as possible but if you do not know the answer, do not invent an it instead say \"I do not know how to answer this question based on the context provided\".\n",
    "As a complement, if asked to corralate the answer with SRE practices or resilient system's design, try to expand on the answer by providing insights extracted from the the site_reliability_engineering and sre_building_secure_and_reliable_systems indexes on how SRE practices and the design of resilient systems could help in the future.\n",
    "When providing insights to the user, always try to provide at least 3 insights and cite the sources.\n",
    "These were previous tasks you completed:\n",
    "Your answer is:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "Complete the objective as best you can. You have access to the following tools:\n",
    "- \"Documentos enviados\"\n",
    "- \"Base de Conhecimento sobre o livro Building Secure and Reliable Systems do Google\"\n",
    "- \"Base de Conhecimento sobre o livro Site Reliability Engineering do Google\"\n",
    "The user question is: {user_input}\n",
    "\n",
    "You are helpful and cheerful assistant who is capable of providing insightful and precise answers to user's question based on the information available in the faiss_uploaded_docs index.\n",
    "You try to be as helpeful as possible but if you do not know the answer, do not invent an it instead say \"I do not know how to answer this question based on the context provided\".\n",
    "As a complement, if asked to corralate the answer with SRE practices or resilient system's design, try to expand on the answer by providing insights extracted from the the site_reliability_engineering and sre_building_secure_and_reliable_systems indexes on how SRE practices and the design of resilient systems could help in the future.\n",
    "When providing insights to the user, always try to provide at least 3 insights and cite the sources.\n",
    "These were previous tasks you completed:\n",
    "Your answer is:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template = f\"\"\"\n",
    "according to my question {user_input}, can you provide a detailed explanation of the event that happened? with additional insights on how SRE practices and the design of resilient systems could help in the future?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=system_template, input_variables=['user_input'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=system_template),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"What happened on Elastic Cloud?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not know how to answer this question based on the context provided. Could you please provide more information or context about what happened on Elastic Cloud?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente = initialize_agent(\n",
    "        agent='chat-zero-shot-react-description',\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        verbose=True,\n",
    "        max_interactions=3,\n",
    "        early_stopping_method='generate',\n",
    "        handle_parsing_errors=True,\n",
    "        memory=memory,\n",
    "    )\n",
    "\n",
    "\n",
    "d = agente.run(prompt.format_prompt(user_input=\"What happened on Elastic Cloud\"))\n",
    "#print(d[0]['messsage'])\n",
    "#return d[0][\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = prompt.format_prompt(user_input=\"What happened on Elastic Cloud\")\n",
    "\n",
    "agente.run(d)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
